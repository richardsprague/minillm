# MinillM

[![CI/CD Pipeline](https://github.com/richardsprague/minillm/actions/workflows/ci.yml/badge.svg)](https://github.com/richardsprague/minillm/actions/workflows/ci.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

A production-ready, modular transformer-based language model implementation based on LLaMA architecture. Built for scalability, performance, and ease of deployment.

## ‚ú® Features

- **üèóÔ∏è Modular Architecture**: Clean separation of concerns with configurable components
- **‚ö° High Performance**: torch.compile, quantization, FlashAttention support
- **üåê Production Ready**: FastAPI web server, Docker support, CI/CD pipeline
- **üîß Hardware Adaptive**: Seamless deployment from Mac M2 (8GB) to Linux GPU servers
- **üìä Monitoring**: Weights & Biases, TensorBoard integration
- **üéØ Easy to Use**: Simple CLI tools and web interface

## üöÄ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/richardsprague/minillm.git
cd minillm

# Install in development mode
pip install -e ".[dev]"

# Or install for production
pip install -e .
```

### Download Model

Download the pretrained model (505M parameters) from [Google Drive](https://drive.google.com/file/d/1CHAgimS47Y34nvUBBpugoSL1FfJ_KVB4/view?usp=sharing) and extract to the parent directory.

### Quick Chat

```bash
# Start interactive chat
llm-chat

# Or with custom settings
llm-chat --temperature 0.8 --max-length 150 --compile
```

### Web Interface

```bash
# Start web server
llm-serve --port 8000

# Open http://localhost:8000 in your browser
```

## üìã Model Card

### Model Details

- **Architecture**: LLaMA-style decoder-only transformer with Grouped Query Attention
- **Parameters**: 505M (1024 hidden dim, 24 layers, 16 heads, 4096 FFN dim)
- **Context Length**: 128-4096 tokens (configurable)
- **Vocabulary**: 50K BPE tokens
- **Precision**: FP32/FP16/BF16 support
- **Special Features**: Chain-of-thought reasoning with thinking tokens

### Training Data

- **Pretraining**: Fineweb-edu (~15B tokens) + BAAI/CCI4.0-M2-CoT-v1 (~2B tokens)
- **Finetuning**: Ultrachat, Google Natural Questions, Smoltalk
- **Answer Generation**: Qwen3:8b

### Performance

- **Memory Usage**: ~2GB FP32, ~1GB FP16, ~500MB INT8
- **Speed**: ~50 tokens/sec on M2 Mac, ~150 tokens/sec on RTX 3090
- **Quality**: Competitive with similar-sized models on common benchmarks

## üèóÔ∏è Architecture

### Core Components

```
minillm/
‚îú‚îÄ‚îÄ config.py          # Unified configuration with Pydantic
‚îú‚îÄ‚îÄ models.py          # Transformer model implementation  
‚îú‚îÄ‚îÄ layers.py          # Modular transformer layers
‚îú‚îÄ‚îÄ tokenizer.py       # Tokenizer management
‚îú‚îÄ‚îÄ generation.py      # Text generation utilities
‚îú‚îÄ‚îÄ optimization.py    # Performance optimizations
‚îú‚îÄ‚îÄ server.py          # FastAPI web server
‚îú‚îÄ‚îÄ cli.py            # Command-line interface
‚îî‚îÄ‚îÄ utils.py          # Utility functions
```

### Key Features

- **Grouped Query Attention (GQA)**: Efficient attention mechanism
- **RoPE**: Rotary positional embedding for better length extrapolation
- **SwiGLU**: Swish-gated linear units in feed-forward networks
- **RMS Normalization**: Root Mean Square layer normalization
- **KV Caching**: Efficient inference with cached attention states

## ‚öôÔ∏è Configuration

All settings are managed through `config.yaml`:

```yaml
model:
  dim: 1024                  # Hidden dimension (actual model size)
  n_layers: 24               # Number of transformer layers  
  n_heads: 16                # Number of attention heads
  ffn_dim: 4096              # Feed-forward network dimension
  vocab_size: 50000          # Vocabulary size

performance:
  compile_model: false       # torch.compile (disabled for compatibility)
  use_quantization: false    # Enable INT8/INT4 quantization  
  use_flash_attention: false # Enable FlashAttention

generation:
  temperature: 0.7           # Sampling temperature
  top_p: 0.9                # Nucleus sampling
  max_length: 100           # Max generation length
```

## üîß Performance Optimizations

### CPU Optimizations

```bash
# Install performance dependencies
pip install ".[performance]"

# Enable Intel optimizations (Intel CPUs)
export USE_INTEL_EXTENSION=1

# Quantization for memory efficiency
llm-chat --quantize --quantization-bits 8
```

### GPU Optimizations

```bash
# Enable compilation and mixed precision
llm-chat --compile --dtype bfloat16

# FlashAttention for long sequences
pip install flash-attn
# Set use_flash_attention: true in config.yaml
```

### Memory Usage

| Configuration | Memory Usage | Speed | Quality |
|--------------|-------------|-------|---------|
| FP32 | ~2GB | Baseline | Best |
| FP16 | ~1GB | 1.5x faster | Excellent |
| BF16 | ~1GB | 1.5x faster | Excellent |
| INT8 | ~500MB | 2-3x faster | Very Good |
| INT4 | ~250MB | 3-4x faster | Good |

## üåê Deployment

### Local Development

```bash
# Start development server with hot reload
uvicorn minillm.server:app --reload --port 8000
```

### Production Deployment

```bash
# Build and run with Docker
docker build -t minillm .
docker run -p 8000:8000 minillm

# Or deploy to cloud platforms
# Supports Heroku, Railway, Fly.io, etc.
```

### Netlify Frontend

The web interface in `web/static/` can be deployed to Netlify:

1. Update API endpoint in `index.html`
2. Deploy the `web/static/` directory
3. Configure your backend API URL

## üîß Recent Fixes & Troubleshooting

### Latest Updates (Jan 2025)

- ‚úÖ **Fixed tensor dimension error**: Resolved critical "Tensors must have same number of dimensions" issue
- ‚úÖ **Improved model compatibility**: Now uses original TransformerModel for 100% checkpoint compatibility  
- ‚úÖ **Enhanced generation quality**: Fixed garbled output by matching original generation logic
- ‚úÖ **Web interface improvements**: Added Purdue logo and resolved 404 errors
- ‚úÖ **Performance optimizations**: Faster model loading and better memory usage

### Common Issues

**Q: Getting "Tensors must have same number of dimensions" error?**  
A: This has been fixed in the latest version. Update to the newest commit.

**Q: Model generates garbled text?**  
A: Ensure you're using the exact model checkpoint from the Google Drive link. The model architecture now matches the original exactly.

**Q: Web interface shows 404 errors?**  
A: Fixed in latest version with proper static file serving and favicon support.

**Q: Slow model loading?**  
A: The model now uses the original architecture for faster loading. Disable compilation if needed: `compile_model: false`

## üõ†Ô∏è Development

### Setup

```bash
# Install development dependencies
pip install -e ".[dev]"

# Install pre-commit hooks
pre-commit install

# Run tests
pytest

# Format code
ruff format minillm/
ruff check minillm/ --fix
```

### Adding Features

1. Update configuration in `config.py`
2. Implement feature in appropriate module
3. Add CLI interface if needed
4. Update tests and documentation
5. Run CI pipeline

## üìä Monitoring

### Weights & Biases

```yaml
logging:
  use_wandb: true
  wandb_project: "minillm"
```

### TensorBoard

```bash
# Enable in config.yaml
logging:
  use_tensorboard: true

# View logs
tensorboard --logdir logs/tensorboard
```

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- **LLaMA Architecture**: Meta AI for the original transformer design
- **Training Data**: Fineweb-edu and BAAI teams for high-quality datasets
- **Optimization Techniques**: FlashAttention, torch.compile, and quantization communities
- **Open Source Libraries**: PyTorch, Transformers, FastAPI, and many others

## üìö Citation

If you use MinillM in your research or projects, please cite:

```bibtex
@software{minillm2025,
  author = {Richard Sprague},
  title = {MinillM: A Production-Ready Transformer Language Model},
  url = {https://github.com/richardsprague/minillm},
  year = {2025}
}
```

---

**Note**: This model is for research and educational purposes. Please ensure compliance with applicable laws and ethical guidelines when deploying in production environments.