# LLM
Transfomer-based language model.
 
Pretrained on Fineweb-edu (~15b tokens) and BAAI/CCI4.0-M2-CoT-v1 (~2B tokens)
Finetuned on various questions from datasets like Ultrachat, Google Natural Questions, and Smoltalk. Answers were generated by Qwen3:8b.

Here is the best model so far (505m parameters):
https://drive.google.com/file/d/1CHAgimS47Y34nvUBBpugoSL1FfJ_KVB4/view?usp=sharing

## To run:
See `gemini.md` for detailed setup and execution instructions.

## To finetune:
1. Change path in the dataloader to the path of your dataset
2. Finetune
### Note: If you run out of memory, use transformer_model_llama_june2025_checkpointing. It is slower but uses around 30% less memory