name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, "3.10", "3.11"]

    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Lint with ruff
      run: |
        ruff check minillm/ --output-format=github
        ruff format minillm/ --check
    
    - name: Type check with mypy
      run: |
        mypy minillm/
      continue-on-error: true  # Allow mypy to fail for now
    
    - name: Test configuration loading
      run: |
        python -c "
        from minillm.config import load_config, ModelConfig
        # Test config creation
        config = ModelConfig()
        print(f'Model config: {config.dim}d, {config.n_layers}L, {config.n_heads}H')
        
        # Test config validation
        try:
            config_invalid = ModelConfig(dim=100, n_heads=16)  # Should fail validation
        except Exception as e:
            print(f'Validation working: {e}')
        "
    
    - name: Test tokenizer interface
      run: |
        python -c "
        from minillm.config import PathsConfig, TokensConfig
        from minillm.tokenizer import TokenizerManager
        import tempfile
        import os
        
        # Create mock tokenizer files for testing
        with tempfile.TemporaryDirectory() as tmpdir:
            vocab_file = os.path.join(tmpdir, 'vocab.json')
            merges_file = os.path.join(tmpdir, 'merges.txt')
            
            # Create minimal valid files
            with open(vocab_file, 'w') as f:
                f.write('{\"hello\": 0, \"world\": 1}')
            with open(merges_file, 'w') as f:
                f.write('#version: 0.2\nhello world')
            
            paths_config = PathsConfig(
                model_file='/dev/null',
                tokenizer_dir=tmpdir,
                vocab_file='vocab.json',
                merges_file='merges.txt'
            )
            tokens_config = TokensConfig()
            
            print('Tokenizer interface test passed')
        "
    
    - name: Test model interface
      run: |
        python -c "
        from minillm.config import ModelConfig
        from minillm.models import TransformerModel
        import torch
        
        # Test model creation
        config = ModelConfig(dim=128, n_layers=2, n_heads=4, vocab_size=1000, max_seq_len=64)
        model = TransformerModel(config)
        
        # Test forward pass
        x = torch.randint(0, 1000, (1, 10))
        with torch.no_grad():
            output = model(x)
        
        print(f'Model test passed: input {x.shape} -> output {output.shape}')
        assert output.shape == (1, 10, 1000)
        "

  build-docs:
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install mkdocs mkdocs-material
    
    - name: Generate API documentation
      run: |
        python -c "
        import inspect
        import minillm
        
        # Generate basic API docs
        with open('docs/api.md', 'w') as f:
            f.write('# MinillM API Reference\n\n')
            for name in dir(minillm):
                if not name.startswith('_'):
                    obj = getattr(minillm, name)
                    if inspect.isclass(obj):
                        f.write(f'## {name}\n\n')
                        f.write(f'{obj.__doc__ or \"No documentation available.\"}\n\n')
        "
      continue-on-error: true

  security-scan:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"
    
    - name: Install safety
      run: pip install safety
    
    - name: Run safety check
      run: safety check --json
      continue-on-error: true

  performance-test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Benchmark model performance
      run: |
        python -c "
        from minillm.config import ModelConfig
        from minillm.models import TransformerModel
        from minillm.optimization import estimate_memory_usage
        import torch
        import time
        
        # Small model for CI
        config = ModelConfig(dim=128, n_layers=2, n_heads=4, vocab_size=1000, max_seq_len=64)
        model = TransformerModel(config)
        model.eval()
        
        # Memory estimation
        memory_est = estimate_memory_usage(model, seq_len=64, batch_size=1)
        print(f'Estimated memory usage: {memory_est[\"total_memory_mb\"]:.1f} MB')
        
        # Speed benchmark
        x = torch.randint(0, 1000, (1, 10))
        
        # Warmup
        with torch.no_grad():
            for _ in range(5):
                _ = model(x)
        
        # Benchmark
        start_time = time.time()
        with torch.no_grad():
            for _ in range(100):
                _ = model(x)
        
        elapsed = time.time() - start_time
        tokens_per_sec = (100 * 10) / elapsed
        print(f'Inference speed: {tokens_per_sec:.1f} tokens/sec')
        "

  check-format:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"
    
    - name: Install ruff
      run: pip install ruff
    
    - name: Check formatting
      run: ruff format minillm/ --check --diff
    
    - name: Check imports
      run: ruff check minillm/ --select I --diff